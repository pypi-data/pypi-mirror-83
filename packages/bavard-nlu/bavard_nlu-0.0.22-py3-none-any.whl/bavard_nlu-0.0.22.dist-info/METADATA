Metadata-Version: 2.0
Name: bavard-nlu
Version: 0.0.22
Summary: A library and CLI for NLP tasks
Home-page: https://github.com/bavard-ai/bavard-nlu
Author: Bavard AI, LLC
Author-email: dev@bavard.ai
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: transformers (>=3.3.1)
Requires-Dist: tensorflow (>=2.2.0)
Requires-Dist: scikit-learn (>=0.23.1)
Requires-Dist: tensorflow-probability (==0.10.1)
Requires-Dist: uncertainty-metrics (==0.0.81)
Requires-Dist: fire (==0.3.1)

# bavard-nlu

## Releasing The Package

Releasing the package is automatically handled by CI, but three steps must be taken to trigger a successful release:

1. Increment the `VERSION` variable in `setup.py` to the new desired version (e.g. `VERSION="1.1.1"`)
2. Commit and tag the repo with the **exact same** value you populated the `VERSION` variable with (e.g. `git tag 1.1.1`)
3. Push the commit and tag to remote. These can be done together using: `git push --atomic origin <branch name> <tag>`

CI will then release the package to pypi with that version once the commit and tag are pushed.

## Local Development

There is a convenience CLI for training, evaluating, predicting, and interacting with NLU models. Too see the CLI documentation:

```
python3 -m bavard_nlu.cli --help
```

You can also view the documentation for a sub-command for example:

```
python3 -m bavard_nlu.cli train --help
```

## Testing Locally

The tests for this repo consist of functional tests for catching bugs/code regressions, and validation tests for catching model performance regressions. The validation tests take much longer to execute. Both are run in CI. To run just the functional tests:

```
python3 -m unittest discover test/functional
```


