description ophidia (
    kind = 'main' and
    short = 'The Ophidia analytics platform.' and
    content = 'The Ophidia analytics platform.

Webpage: http://ophidia.cmcc.it'
)

network public (
  outbound = 'yes' and
  # IM port
  outports contains '8800/tcp' and
  # Ophidia ports
  outports contains '8080/tcp' and
  outports contains '11732/tcp' and
  outports contains '443/tcp'
)


network private ()

system front (
  cpu.count>=2 and
  memory.size>=4096m and
  net_interface.0.connection = 'private' and
  net_interface.0.dns_name = 'oph-server' and
  net_interface.1.connection = 'public' and
  net_interface.1.dns_name = 'oph-serverrpublic' and
  queue_system = 'slurm' and
  ec3_templates contains 'ophidia' and
  disk.0.applications contains (name = 'ansible.modules.OphidiaBigData.ophidia-cluster') and
  disk.0.applications contains (name = 'ansible.modules.grycap.clues') and
  disk.0.applications contains (name = 'ansible.modules.grycap.im') and
  disk.1.size=25g and
  disk.1.fstype='ext4' and
  disk.1.mount_path='/data'
)

system wn (
cpu.count>=2 and
memory.size>=4g and
net_interface.0.connection = 'private' and
net_interface.0.dns_name = 'oph-io#N#'
)

configure front (
@begin
---
 - vars:
      # Variables needed for CLUES
      SYSTEMS:
        ec3_jpath: /system/*
      NNODES: '{{ SYSTEMS | selectattr("ec3_max_instances_max", "defined") | sum(attribute="ec3_max_instances_max") }}'
      AUTH:
        ec3_xpath: /system/front/auth

   pre_tasks:
    - name: stop service firewalld
      systemd: name=firewalld state=stopped
    - name: Creates NFS shared directory
      file: path=/data state=directory owner=root group=root

   roles:
    - role: 'OphidiaBigData.ophidia-cluster' 
      node_type: 'server'
      deploy_type: 'complete'
      server_hostname: "{{ansible_hostname}}"
      io_ips: "{{ groups['wn']|map('extract', hostvars, ['ansible_default_ipv4', 'address'])|list if 'wn' in groups else []}}"
      private_server_ip: "{{ IM_NODE_PRIVATE_IP }}"
      public_server_ip: "{{ IM_NODE_PUBLIC_IP }}"
      nfs_subnet: "oph-*.localdomain"
      mysql_subnet: "oph-%.localdomain"
      io_node_number: "{{ NNODES }}"
      io_cpus: '{{IM_INFRASTRUCTURE_RADL|json_query("[?id == ''wn''].cpu_count_min|[0]")}}'

    - role: 'grycap.im'

    - role: 'grycap.clues'
      auth: '{{AUTH}}'
      clues_queue_system: 'slurm'
      max_number_of_nodes: '{{ NNODES }}'
      vnode_prefix: 'wn'
      clues_config_options:
        - { section: 'scheduling', option: 'IDLE_TIME', value: '300' }
        - { section: 'scheduling', option: 'RECONSIDER_JOB_TIME', value: '60' }
        - { section: 'monitoring', option: 'MAX_WAIT_POWERON', value: '3000' }
        - { section: 'monitoring', option: 'MAX_WAIT_POWEROFF', value: '600' }
        - { section: 'monitoring', option: 'PERIOD_LIFECYCLE', value: '10' }
        - { section: 'monitoring', option: 'PERIOD_MONITORING_NODES', value: '2' }
        - { section: 'client', option: 'CLUES_REQUEST_WAIT_TIMEOUT', value: '3000' }
        - { section: 'scheduling', option: 'EXTRA_NODES_FREE', value: '1' }
        - { section: 'scheduling', option: 'SCHEDULER_CLASSES', value: 'clueslib.schedulers.CLUES_Scheduler_PowOn_Requests, clueslib.schedulers.CLUES_Scheduler_Reconsider_Jobs, clueslib.schedulers.CLUES_Scheduler_PowOff_IDLE, clueslib.schedulers.CLUES_Scheduler_PowOn_Free' }

   tasks:
    - name: stop service firewalld
      systemd: name=firewalld state=stopped enabled=no

    # Add hooks to disable nodes until Ophidia is installed
    - name: Create Hooks configuration file for CLUES2
      copy: src=/etc/clues2/conf.d/hooks.cfg-example dest=/etc/clues2/conf.d/hooks.cfg force=no
      notify: restart cluesd

    - name: Create Hooks configuration file for CLUES2
      copy:
        content: |
          #!/bin/bash
          if [ $2 -eq 1 ]
          then
              scontrol update NodeName=$1 State=DRAIN Reason="Installing"
          else
              echo "Do not disable node"
          fi
        dest: /etc/clues2/scripts/node_drain.sh
        force: no
        mode: '755'

    - name: Set POST_POWERON hook value
      ini_file: dest=/etc/clues2/conf.d/hooks.cfg section=hooks option=POST_POWERON value="./node_drain.sh"
      notify: restart cluesd

@end
)

configure wn (
@begin
---
 - roles:
    - role: 'OphidiaBigData.ophidia-cluster'
      node_type: 'io'
      deploy_type: 'complete'
      server_hostname: "{{ hostvars[groups['front'][0]]['ansible_hostname'] }}"
      io_ips: "{{ groups['wn']|map('extract', hostvars, ['ansible_default_ipv4', 'address'])|list if 'wn' in groups else []}}"
      private_server_ip: "{{ hostvars[groups['front'][0]]['IM_NODE_PRIVATE_IP'] }}"
      public_server_ip: "{{ hostvars[groups['front'][0]]['IM_NODE_PUBLIC_IP'] }}"
      nfs_subnet: "oph-*.localdomain"
      mysql_subnet: "oph-%.localdomain"
      io_node_number: "{{ groups['wn']|length if 'wn' in groups else 1}}"
      io_cpus: '{{IM_INFRASTRUCTURE_RADL|json_query("[?id == ''wn''].cpu_count_min|[0]")}}'

   tasks:
    - name: stop service firewalld
      systemd: name=firewalld state=stopped enabled=no

    - name: Resume node
      command: scontrol update NodeName={{ ansible_hostname }} State=RESUME
      ignore_errors: yes

@end
)

deploy front 1
deploy wn 1
