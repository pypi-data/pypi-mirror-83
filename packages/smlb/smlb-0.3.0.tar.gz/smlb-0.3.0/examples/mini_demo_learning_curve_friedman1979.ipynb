{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `smlb` mini demonstration:<br>Compare different learners on the same dataset\n",
    "\n",
    "Scientific Machine Learning Benchmark:<br>\n",
    "A benchmark of regression models in chem- and materials informatics.<br>\n",
    "2019-2020, Citrine Informatics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import smlb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset: Friedman (1979)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset. Use tab completion to find the right import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.datasets.synthetic.friedman_1979.friedman_1979 import Friedman1979Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about the dataset.\n",
    "Note that references are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Friedman1979Data.__doc__) # using `print` instead of `help` avoids clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Friedman1979Data(dimensions=5)  # ignore uncorrelated 6th dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling validation and training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homogeneous measurement of error, we sample the validation set on a regular grid in 5 dimensions.<br>\n",
    "Note that specification of the pseudo-random number generator seed `rng` is mandatory as `smlb` takes reproducibility seriously:<br>\n",
    "results will be deterministic for a given seed, even if running in a distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = smlb.GridSampler(size=7**5, domain=[0,1], rng=0) # 16807 samples on a 7 x 7 x ... x 7 grid; original hypercube domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training sets are of increasing size, equi-distant in log-space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [10, 19, 37, 73, 143, 278, 542, 1056, 2055, 4000];\n",
    "training_sets = tuple(smlb.GridSampler(size=size, domain=[0,1], rng=0) for size in training_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners: Gaussian process regression and random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `scikit-learn` implementations, which `smlb` wraps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.learners.scikit_learn.gaussian_process_regression_sklearn import GaussianProcessRegressionSklearn\n",
    "learner_gpr_skl = GaussianProcessRegressionSklearn(random_state=0) # default is Gaussian kernel\n",
    "\n",
    "from smlb.learners.scikit_learn.random_forest_regression_sklearn import RandomForestRegressionSklearn\n",
    "learner_rf_skl = RandomForestRegressionSklearn(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The workflow: bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the appropriate workflow for benchmarking several learners on a single dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.workflows.learning_curve_regression import LearningCurveRegression\n",
    "workflow = LearningCurveRegression(data=dataset, training=training_sets, validation=validation_set, learners=[learner_rf_skl, learner_gpr_skl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>What happened?</b><br>\n",
    "    <tt>smlb</tt> has detected that there is overlap between the samples in the first training set and the validation set.<br>\n",
    "    In fact, the whole n=10 training set is a subset of the validation set because it lies on a subgrid.<br>\n",
    "    <i>Such overlap can cause arbitrarily wrong performance estimates.</i><br>\n",
    "    <t>smlb</t> emphasizes correctness, and guards against this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The workflow #2: correct sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample the training sets randomly, but keep the validation set on a grid as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = tuple(smlb.RandomVectorSampler(size=size, rng=0) for size in training_sizes) # dataset domain is used by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the workflow again. We also tell it to produce a learning curve that we can modify within the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "learning_curve = smlb.LearningCurvePlot(target=ax, axes_labels=(\"training set size\", \"root mean-squared error\"))\n",
    "workflow = LearningCurveRegression(data=dataset, training=training_sets, validation=validation_set, \n",
    "                                   learners=[learner_rf_skl, learner_gpr_skl], evaluations=[learning_curve,])\n",
    "workflow.run()\n",
    "ax.legend([\"Random forest\", \"Gaussian process\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows<br>\n",
    "* an anomaly of the Gaussian process at intermediate training set sizes, likely due to hyperparameter optimization,\n",
    "* better performance of the Gaussian process compared to the random forest on this *smooth* synthetic dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smlb",
   "language": "python",
   "name": "smlb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
