{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `smlb` mini demonstration:<br>Compare different learners on multiple datasets\n",
    "\n",
    "Scientific Machine Learning Benchmark:<br>\n",
    "A benchmark of regression models in chem- and materials informatics.<br>\n",
    "2019-2020, Citrine Informatics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration shows how a panel of learning curves can visualize performance of multiple learners on multiple datasets.<br>\n",
    "Such a panel can be used for assessment of learners and as a dashboard for regular automated performance tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import IPython\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import smlb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Random\" numbers are generated deterministically using pseudo-random number generators (PRNG).<br>\n",
    "`smlb` takes reproducibility seriously: Given identical software and hardware, results will be deterministic for a given seed,<br>\n",
    "even if running asynchronously, in parallel, or in a distributed environment. This supports reproducibility.<br>\n",
    "As a consequence, PRNG seeds must be deterministically created and specified.<br> \n",
    "`smlb` supports this by providing a `split` method that creates new seeds for PRNGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = smlb.Random(rng=0)  # pseudo-random number generator\n",
    "seeds = list(np.flip(prng.random.split(100)))  # for simplicity, just create a sufficiently large number of seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we use six datasets that come with `smlb`, three experimental and three synthetic ones.<br>\n",
    "Use tab-completion to see all available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.datasets.experimental.band_gaps_sc73.band_gaps_sc73 import BandGapsStrehlowCook1973Dataset\n",
    "from smlb.datasets.experimental.superconductors_citrine16.superconductors_citrine16 import SuperconductorsCitrine2016Dataset\n",
    "from smlb.datasets.experimental.clean_energy_project.clean_energy_project import CleanEnergyProjectDataset\n",
    "\n",
    "from smlb.datasets.synthetic.friedman_1979.friedman_1979 import Friedman1979Data\n",
    "from smlb.datasets.synthetic.friedman_silverman_1989.friedman_silverman_1989 import FriedmanSilverman1989Data\n",
    "from smlb.datasets.synthetic.schwefel26_1981.schwefel26_1981 import Schwefel261981Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset has its own characteristics and is different from the others.<br>\n",
    "To get information about any of them, simply print its docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(BandGapsStrehlowCook1973Dataset.__doc__, BandGapsStrehlowCook1973Dataset.__init__.__doc__)\n",
    "# print(SuperconductorsCitrine2016Dataset.__doc__, SuperconductorsCitrine2016Dataset.__init__.__doc__)\n",
    "# print(CleanEnergyProjectDataset.__doc__, CleanEnergyProjectDataset.__init__.__doc__)\n",
    "\n",
    "# print(Friedman1979Data.__doc__, Friedman1979Data.__init__.__doc__)\n",
    "# print(FriedmanSilverman1989Data.__doc__, FriedmanSilverman1989Data.__init__.__doc__)\n",
    "# print(Schwefel261981Data.__doc__, Schwefel261981Data.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrange the datasets in a 2 x 3 array, the same way the panel will present them.<br>\n",
    "Any parametrization takes place at initialization (use shift+tab for information on arguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change filename to where your local copy of the dataset exists\n",
    "# if you don't have the dataset, download it from https://figshare.com/articles/moldata_csv/9640427 (0.56 GB)\n",
    "cep_filename = os.path.join(os.path.expanduser(\"~\"), \"smlb-local/datasets/clean_energy_project_moldata.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = np.asarray([\n",
    "    [\n",
    "        BandGapsStrehlowCook1973Dataset(filter_='bg', join=1, \n",
    "            samplef=lambda e: e['formula'], labelf=np.median),\n",
    "        SuperconductorsCitrine2016Dataset(\n",
    "            process=True,\n",
    "            join=True,\n",
    "            filter_=lambda e: not any(e[\"flagged_formula\"]),\n",
    "            samplef=lambda e: e[\"formula\"],\n",
    "            labelf=lambda tc: np.median(tc),\n",
    "        ),\n",
    "        CleanEnergyProjectDataset(\n",
    "            source=cep_filename, \n",
    "            join=True,\n",
    "            samplef=lambda e: e['formula'],  # reduce to stoichiometry\n",
    "            labelf=lambda e: np.median(e['gap']),  # predict band gap\n",
    "        ),\n",
    "    ],\n",
    "    [\n",
    "        Friedman1979Data(dimensions=6),\n",
    "        FriedmanSilverman1989Data(dimensions=10),\n",
    "        Schwefel261981Data(dimensions=15),\n",
    "    ],\n",
    "])\n",
    "\n",
    "dataset_names = [\n",
    "    ['Strehlow & Cook (1973)', 'Citrine Superconductors (2016)', 'Clean Energy Project (2019)'],\n",
    "    ['Friedman (1979)', 'Friedman & Silverman (1989)', 'Schwefel #26 (1981)'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental datasets require explicit featurization for use with learners that expect numerical arrays as inputs.<br>\n",
    "The synthetic datasets are all vector spaces, and can be used as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.features.matminer_composition import MatminerCompositionFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "        \n",
    "            features = np.asarray([\n",
    "                [\n",
    "                    MatminerCompositionFeatures(ionic_fast=True),  # required for non-integer formulas\n",
    "                    MatminerCompositionFeatures(ionic_fast=True),  # required for non-integer formulas\n",
    "                    MatminerCompositionFeatures(ionic_fast=True),  # abuse for molecules...\n",
    "                ],\n",
    "                [\n",
    "                    smlb.IdentityFeatures(),\n",
    "                    smlb.IdentityFeatures(),\n",
    "                    smlb.IdentityFeatures(),\n",
    "                ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves are based on increasing training set sizes, ususally equi-distant in log-space.<br>\n",
    "Since their size depends on number of samples in a dataaset, we use individual sizes.<br>\n",
    "For validation we use a hold-out set containing 20% of all samples.\n",
    "\n",
    "Arbitrarily many samples can be drawn from the synthetic datasets.<br>\n",
    "We choose reasonably small training set sizes, motivated by our interest in small-data scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_fraction, num_train_min, num_training_sets, max_size = 0.2, 10, 6, 500\n",
    "\n",
    "def calc_split_sizes(data):\n",
    "    \"\"\"Calculate validation and training set sizes for a dataset\"\"\"\n",
    "    num_samples = data if isinstance(data, int) else data.num_samples\n",
    "    num_validation = int(np.floor(validation_fraction * num_samples))\n",
    "    num_validation = min(num_validation, max_size)  # cap validation set size\n",
    "    num_train_max = np.floor((1 - validation_fraction) * num_samples)\n",
    "    num_train_max = min(num_train_max, max_size)\n",
    "    num_training = np.logspace(np.log10(num_train_min), \n",
    "        np.log10(num_train_max), num_training_sets, dtype=int)\n",
    "    return num_validation, num_training\n",
    "\n",
    "# print(calc_split_sizes(datasets[0][0]))\n",
    "# print(calc_split_sizes(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes = np.asarray([\n",
    "    [calc_split_sizes(data) for data in datasets[0]],  \n",
    "    [calc_split_sizes(650) for _ in datasets[1]],\n",
    "])\n",
    "\n",
    "split_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different samplers for finite datasets and vector-space datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samplers = np.array([\n",
    "    [\n",
    "        smlb.RandomSubsetSampler(size=m, rng=seeds.pop())\n",
    "        for m in split_sizes[0, :, 0]\n",
    "    ],\n",
    "    [\n",
    "        smlb.RandomVectorSampler(size=m, rng=seeds.pop())\n",
    "        for m in split_sizes[1, :, 0]\n",
    "    ],\n",
    "])\n",
    "\n",
    "training_samplers = np.array([\n",
    "    [\n",
    "        [smlb.RandomSubsetSampler(size=n, rng=seeds.pop()) for n in nn]\n",
    "        for nn in split_sizes[0, :, 1]\n",
    "    ],\n",
    "    [\n",
    "        [smlb.RandomVectorSampler(size=n, rng=seeds.pop()) for n in nn]\n",
    "        for nn in split_sizes[1, :, 1]\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare `scikit-learn` random forests with `lolo` ones.<br>\n",
    "In this benchmark, we use default parametrizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.learners.scikit_learn.random_forest_regression_sklearn import RandomForestRegressionSklearn\n",
    "from smlb.learners.scikit_learn.extremely_randomized_trees_regression_sklearn import ExtremelyRandomizedTreesRegressionSklearn\n",
    "from smlb.learners.scikit_learn.gradient_boosted_trees_regression_sklearn import GradientBoostedTreesRegressionSklearn\n",
    "\n",
    "from smlb.learners.lolo.random_forest_regression_lolo import RandomForestRegressionLolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = [\n",
    "    RandomForestRegressionSklearn(random_state=seeds.pop()), \n",
    "    ExtremelyRandomizedTreesRegressionSklearn(random_state=seeds.pop()),\n",
    "    GradientBoostedTreesRegressionSklearn(random_state=seeds.pop()),\n",
    "    RandomForestRegressionLolo(),  # currently no support for passing prng seed\n",
    "]\n",
    "\n",
    "learner_names = ['random forest', 'extra-trees', 'gradient-boost', 'lolo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlb.workflows.learning_curve_regression import LearningCurveRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to have `matplotlib` code and `smlb` code for the workflow in one block of code<br>\n",
    "is a limitation of Jupyter notebook default settings and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=datasets.shape[0], ncols=datasets.shape[1], \n",
    "    squeeze=False, figsize=(16,9))\n",
    "\n",
    "mpl.rcParams['axes.titleweight'] = 'bold'\n",
    "cfg = smlb.PlotConfiguration(font_size=10)\n",
    "\n",
    "plots = [\n",
    "    [\n",
    "        smlb.LearningCurvePlot(\n",
    "            target=ax[row][col], \n",
    "            rectify=True,\n",
    "            configuration=cfg,\n",
    "            axes_labels=(\n",
    "                'training set size' if row == 1 else None, \n",
    "                'Root Mean Squared Error' if col == 0 else None\n",
    "            ),\n",
    "        )\n",
    "        for col in range(datasets.shape[1])\n",
    "    ]\n",
    "    for row in range(datasets.shape[0])\n",
    "]\n",
    "\n",
    "pb = tqdm.tqdm(total=datasets.shape[0]*datasets.shape[1], leave=False)\n",
    "\n",
    "for row in range(datasets.shape[0]):\n",
    "    for col in range(datasets.shape[1]):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "            wf = LearningCurveRegression(\n",
    "                data=datasets[row][col],\n",
    "                features=features[row][col],\n",
    "                training=training_samplers[row][col],\n",
    "                validation=validation_samplers[row][col],\n",
    "                learners=learners, \n",
    "                evaluations=[plots[row][col]]\n",
    "            )\n",
    "            \n",
    "            ax[row][col].set_title(dataset_names[row][col])\n",
    "            \n",
    "            wf.run()\n",
    "            \n",
    "            pb.update(1)\n",
    "\n",
    "ax[-1,-1].legend(learner_names, fontsize=12, loc=(1.05,0.05))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection of the learning curves is most useful for single analyses,<br>\n",
    "but repeated evaluation calls for more easily accessible presentations.\n",
    "\n",
    "`smlb` allows extraction of auxiliary information such as the offset and slope of the learning curves,<br>\n",
    "which can then be displayed in a table for manual inspection at a glance or automated evaluation for a dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[0][0].auxiliary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = np.asfarray([\n",
    "    [\n",
    "        [\n",
    "            (entry['offset'], entry['slope']) \n",
    "            for entry in plot.auxiliary['asymptotic_fits']\n",
    "        ]\n",
    "        for plot in row\n",
    "    ]\n",
    "    for row in plots\n",
    "])\n",
    "# fits [ row ] [ col ] [ learner ] [ offset/slope ]\n",
    "\n",
    "fits = np.reshape(fits, newshape=(datasets.shape[0]*datasets.shape[1], len(learners), 2))\n",
    "# fits [ row/col ] [ learner ] [ offset/slope ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = np.asarray(\n",
    "[\n",
    "    np.hstack( (rowcol[:,0], rowcol[:,1]) )\n",
    "    for rowcol in fits\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    table_data, \n",
    "    columns=learner_names + [name + \" \" for name in learner_names], \n",
    "    index=np.asarray(dataset_names).ravel()\n",
    ")\n",
    "\n",
    "with pd.option_context('display.float_format', '{:5.2f}'.format, 'display.width', 9999):\n",
    "    IPython.display.display(IPython.display.HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing to results tables from previous runs, a performance dashboard could be used<br>\n",
    "to regularly monitor integrated performance of machine-learning algorithms on multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.iloc[1,-1] = -0.1\n",
    "\n",
    "def _style_cell(x):\n",
    "    color = 'color: red;'\n",
    "    df1 = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "    df1.iloc[1, -1] = color\n",
    "    return df1\n",
    "\n",
    "#with pd.option_context('display.float_format', '{:5.2f}'.format, 'display.width', 9999):\n",
    "df2s = df2.style.apply(_style_cell, axis=None).format('{:5.2f}')\n",
    "IPython.display.display(IPython.display.HTML(df2s.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
