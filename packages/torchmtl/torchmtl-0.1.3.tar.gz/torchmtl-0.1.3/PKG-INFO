Metadata-Version: 2.1
Name: torchmtl
Version: 0.1.3
Summary: A lightweight module for Multi-Task Learning in pytorch
License: MIT
Author: Christian Bock
Author-email: christian.bock@bsse.ethz.ch
Requires-Python: >=3.7,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Requires-Dist: matplotlib (>=3.3.2,<4.0.0)
Requires-Dist: networkx (>=2.5,<3.0)
Requires-Dist: scipy (>=1.5.3,<2.0.0)
Requires-Dist: torch (>=1.6.0,<2.0.0)
Description-Content-Type: text/markdown

![alt text](https://github.com/chrisby/torchMTL/blob/main/torchmtl_logo.png "torchMTL Logo")    
A lightweight module for Multi-Task Learning in pytorch.

`torchmtl` tries to help you composing modular multi-task architectures with minimal effort. All you need is a list of dictionaries in which you define your layers and how they build on each other. From this, `torchmtl` constructs a meta-computation graph which is executed in each forward pass of the created `MTLModel`. To combine outputs from multiple layers, simple [wrapper functions](https://github.com/chrisby/torchMTL/blob/main/torchmtl/wrapping_layers.py) are provided.

### Installation
`torchmtl` can be installed via `pip`:
```
pip install torchmtl
```

### Quickstart
Assume you want to use two different embeddings of your input, combine them and then solve different prediction tasks.

