# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['text_data', 'text_data.core']

package_data = \
{'': ['*']}

install_requires = \
['ipython', 'numpy', 'text_data_rs>=0.1.0,<0.2.0']

extras_require = \
{'display': ['altair>=4.1.0,<5.0.0', 'pandas']}

setup_kwargs = {
    'name': 'text-data',
    'version': '0.1.0',
    'description': 'A Python package for exploring text data for analysis',
    'long_description': '# Text Data\n\n*A tool for exploring and analyzing text data*\n\n`text_data` is a lightweight, performant, and framework-independent tool for analyzing text data in Python. Requiring only `numpy` and\n`IPython`, it offers a suite of features that make it easy to\nsearch through documents, compute statistics on those documents,\nand visualize your findings.\n\nIts key features are:\n\n- An [inverted positional index](https://nlp.stanford.edu/IR-book/html/htmledition/positional-indexes-1.html) allowing you to efficiently conduct boolean searches across a set of documents, including for arbitrarily long phrases.\n- A set of statistical calculations for analyzing text data, most of which work either as single point estimates or as vectorized calculations across an entire corpus.\n- A flexible design that makes it easy to split up your corpus so you can explore and try to find out what makes different portions of a corpus distinct.\n- An optional dependency that allows you to display data visualizations of your findings.\n\nThe visualizations and analyses you can conduct can run from the simple to the complex. At a simple level, you can compute the counts of individual words, mocking the behavior of `collections.Counter`. But you can also find the TF-IDF scores of every word-document pair in the corpus or manipulate `numpy` arrays to your heart\'s desire. On a visual front, you can do anything from building simple bar charts of word counts to building heatmaps showing the outputs of a machine learning model. This, for instance, shows the cosine distances between the `doc2vec` vectors in different State of the Union Addresses, using [the Kaggle State of the Union Corpus](https://www.kaggle.com/rtatman/state-of-the-union-corpus-1989-2017):\n\n![Heatmap showing the distances between State of the Union speeches for the two Rooesevelt presidents, President Eisenhower, and President Truman](images/recent_sotu_heatmap.png)\n\n## Table of Contents\n\n- [Getting Started](#getting-started)\n- [A More Complete Pitch](#a-more-complete-pitch)\n- [Anti-Pitch](#anti-pitch)\n- [Future Roadmap](#future-roadmap)\n- [Contributing](#contributing)\n- [Credits](#credits)\n\n## Getting Started\n\nThere are two ways to install `text_data`. The minimal version of the tool can be installed with a simple `pip install`:\n\n```bash\npip install text_data\n```\n\nIf you want to install features for visualizing your findings, you\ncan optionally install those using:\n\n```bash\npip install text_data[display]\n```\n\nor\n\n```bash\npoetry add text_data -E display\n```\n\n## A More Complete Pitch\n\nMy idea for this library stemmed from a story I wrote [on the way that politicians message themselves in different platforms](https://coloradosun.com/2020/09/04/cory-gardner-john-hickenlooper-campaign-messaging/).\n\nWhile I was working on that story, I noticed that I was writing a few functions that I felt could be generalized for other text-based analyses. Specifically, I spent a lot of my time writing code to display documents that matched certain phrases and and compute statistics comparing one set of documents to another. In addition, the code that I was writing was painfully inefficient.\n\nThis library aims to address all of these problems. Searching is as easy as collecting a list of raw text and a tokenizer, and creating a corpus. Using parallelized Rust code and an efficient indexing structure, the search results and vectorized statistical calculations return rapidly.\n\nIn addition, the code only relies on `numpy` and `IPython`, and its design is aimed to make it simple to use regardless of what other frameworks or libraries your analysis is using.\n\n## Anti-Pitch\n\nThe biggest drawback of this library is that it is not cache-efficient. All of the matrix calculations currently return `numpy` arrays, which are very sparse and thus pose more of a memory burden than they need to. In addition, the library does not have any support for storing data in SQL. As a result, it\'s not well suited for tasks in which you have large quantities of text data. (Eventually, [I hope to change that](#future-roadmap).)\n\nIn addition, there are some frameworks whose authors have thought long and hard about how to help people explore text. (I\'ve only used it a couple of times in a very exploratory manner, but [`fastai`](https://github.com/fastai/fastai) comes to mind.) If you\'re using one of those libraries already, you might be best off sticking with what you have.\n\n## Future Roadmap\n\nThere are a number of goals I have with `text_data`. First of all, I would love to see its set of supported statistical computations continually expand. If you have an idea of something that should be supported, please file an issue or fork the library and file a pull request.\n\nMore concretely, I want to add the following functionality:\n\n- Sparse matrix support in `scipy`. This will help address some of the current problems the library has with memory efficiency.\n- Support for SQL.\n- Support for word co-occurrence matrix calculations and support for boolean `WITHIN` searches.\n\nAnd I\'m very open to other ideas.\n\n## Contributing\n\nThere are two related code bases for this project. The first is the Python code base, while the second is the Rust code base. Assuming you have Python 3.7+ and `poetry` installed, you should be able to install the Python dependencies with\n\n```bash\npoetry install\n```\n\nIn order to compile the Rust code, you will need to have Rust and\nCargo installed. Installation instructions are available on [Rust\'s website](https://www.rust-lang.org/tools/install). Testing also relies on [`clippy`](https://github.com/rust-lang/rust-clippy), [`rustfmt`](https://github.com/rust-lang/rustfmt), and [`cargo tarpaulin`](https://github.com/xd009642/tarpaulin).\n\nIf you only want to mess around with the Python code and do not want to touch the Rust code, you can run\n\n```bash\npoetry run make lint-py\npoetry run pytest\n```\nto lint and test the Python code. Running\n\n```bash\npoetry run make lint\npoetry run make test\n```\nwill run the full test suite.\n\nOnce you\'ve changed the code, written tests, and added documentation to this project, file a pull request. That will trigger a GitHub action. If the tests and linting passes, I will review the code and decide whether to incorporate the code into the project.\n\n## Credits\n\n- This package was created with [Cookiecutter](https://github.com/audreyr/cookiecutter) and the \n[audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template.\n\n- The structure of the Rust code is heavily modified from, but inspired by, this [inverted index crate in Rust](https://github.com/tikue/inverted_index).\n\n- Some of the statistical functions and indexing functions from this crate were inspired by `scikit`, `pandas`, or `nltk` (in particular, the design of the TF-IDF matrix function, which is modeled after `scikit`\'s `TfidfTransformer`.)\n\n- The visualization functions were modified from examples on [`altair`\'s website](https://altair-viz.github.io/).\n\n- I relied on the design of the positional index from [*Introduction to Information Retrieval*](https://nlp.stanford.edu/IR-book/) by Manning, Raghavan and SchÃ¼tze.\n\n- ["Fightin\' Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict"](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) by Monroe, Colaresi, and Quinn formed the inspiration of some of the statistical computations and of the word frequency graphic.\n\n',
    'author': 'Max Lee',
    'author_email': 'maxbmhlee@gmail.com',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/maxblee/text_data',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'extras_require': extras_require,
    'python_requires': '>=3.7,<4.0',
}


setup(**setup_kwargs)
