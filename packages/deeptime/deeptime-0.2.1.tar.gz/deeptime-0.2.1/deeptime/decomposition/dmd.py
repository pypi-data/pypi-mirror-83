from typing import Tuple

import numpy as np
import scipy

from ..base import Estimator, Model, Transformer
from ..kernels import Kernel
from ..numeric import sort_by_norm


class DMDModel(Model, Transformer):
    r""" Model produced by the :class:`DMD` estimator.
    """

    def __init__(self, eigenvalues: np.ndarray, modes: np.ndarray, mode='exact'):
        r""" Creates a new model instance.

        Parameters
        ----------
        eigenvalues : (n,) ndarray
            The DMD eigenvalues.
        modes : (n, n) ndarray
            The DMD modes.
        mode : str, optional, default='exact'
            The mode of estimation that was used. See :attr:`DMD.available_modes`.
        """
        super().__init__()
        self.eigenvalues = eigenvalues
        self.modes = modes
        self.mode = mode

    def transform(self, data: np.ndarray, **kwargs):
        r""" Transforms an input trajectory by applying the model's captured dynamics.

        Parameters
        ----------
        data : (T, n) ndarray
            Input trajectory
        **kwargs
            Compatibility.

        Returns
        -------
        output_trajectory : (T, n) ndarray

        """
        if self.mode == 'exact':
            modes_adj = np.linalg.pinv(self.modes.T)
            return np.linalg.multi_dot([
                self.modes.T, np.diag(self.eigenvalues), modes_adj, data.T
            ]).T
        else:
            return np.linalg.multi_dot([
                self.modes.T, np.diag(self.eigenvalues), self.modes.conj(), data.T
            ]).T


class DMD(Estimator, Transformer):
    r""" Dynamic mode decomposition :cite:`dmd-schmid2010dynamic` estimator.

    There are two supported modes:

    * `standard`, which produces "projected" DMD modes (following the original formulation of DMD),
    * `exact`, which produces DMD modes that do not required ordered data but just
      matched pairs of data :cite:`dmd-tu2013dynamic`.

    Notes
    -----
    In standard DMD, one considers a temporally ordered list of
    data vectors :math:`(z_0,\ldots,z_T)\in\mathbb{R}^{T\times d}`. The data is split into the pair

    .. math::

        X = (z_0, \ldots, z_{T-1}),\quad Y=(z_1,\ldots, z_T).

    If the mode is `exact`, the list does not need to be temporally ordered but just the pairs :math:`(X_i, Y_i)`
    have to match. The underlying assumption is that the data are generated by a linear relationship

    .. math::

        z_{t+1} = A z_t

    for some matrix :math:`A`.

    The so-called DMD modes and eigenvalues are then the (potentially scaled) eigenvectors and eigenvalues of :math:`A`.

    References
    ----------
    .. bibliography:: /references.bib
        :style: unsrt
        :filter: docname in docnames
        :keyprefix: dmd-
    """

    available_modes = 'exact', 'standard'  #: The available estimation modes.
    available_drivers = 'numpy', 'scipy'  #: The available drivers.

    def __init__(self, mode='exact', rank=None, driver='scipy'):
        r"""Creates a new DMD estimator.

        Parameters
        ----------
        mode : str
            The estimation mode, see :attr:`available_modes` for available modes.
        rank : int or None, optional, default=None
            Truncation of the rank after performing SVD.
        driver : str, default='numpy'
            Which package to use for the SVD. Defaults to numpy, can also be 'scipy'.
        """
        super().__init__()
        if mode not in DMD.available_modes:
            raise ValueError(f"Invalid mode {mode}, must be one of {DMD.available_modes}.")
        if driver not in DMD.available_drivers:
            raise ValueError(f"Invalid driver {driver}, must be one of {DMD.available_drivers}.")
        self.mode = mode
        self.rank = rank
        self.driver = driver

    def _svd(self, mat, **kw):
        if self.driver == 'numpy':
            return np.linalg.svd(mat, **kw)
        elif self.driver == 'scipy':
            return scipy.linalg.svd(mat, **kw)

    def _eig(self, mat, **kw):
        if self.driver == 'numpy':
            return np.linalg.eig(mat, **kw)
        elif self.driver == 'scipy':
            return scipy.linalg.eig(mat, **kw)

    def fit(self, data: Tuple[np.ndarray, np.ndarray], **kwargs):
        X, Y = data[0].T, data[1].T  # per convention arrays are [T, d] so here we transpose them

        U, s, Vt = self._svd(X, full_matrices=False)
        if self.rank is not None:
            rank = min(self.rank, U.shape[1])
            U = U[:, :rank]
            s = s[:rank]
            Vt = Vt[:rank]
        V = Vt.conj().T
        S_inv = np.diag(1 / s)
        A = np.linalg.multi_dot([U.conj().T, Y, V, S_inv])

        eigenvalues, eigenvectors = self._eig(A)
        order = np.argsort(eigenvalues)
        eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]

        if self.mode == 'exact':
            dmd_modes = np.linalg.multi_dot([Y, V, S_inv, eigenvectors, np.diag(1 / eigenvalues)])
        elif self.mode == 'standard':
            dmd_modes = U @ eigenvectors
        else:
            raise ValueError('Only exact and standard DMD available.')

        self._model = DMDModel(eigenvalues, dmd_modes.T)
        return self

    def fetch_model(self):
        r""" Yields the estimated model if :meth:`fit` was called.

        Returns
        -------
        model : DMDModel or None
            The model or None.
        """
        return self._model

    def transform(self, data, **kwargs):
        r""" Predicts the propagated data given input.

        Parameters
        ----------
        data : (T, d) np.ndarray
            Input data

        Returns
        -------
        result : (T, d) np.ndarray
            Propagated input data
        """
        return self.fetch_model().transform(data, **kwargs)


class KernelEDMDModel(Model):
    def __init__(self, P, K):
        super().__init__()
        self.P = P
        self.K = K


class KernelEDMDEstimator(Estimator):

    def __init__(self, kernel: Kernel, epsilon: float = 0., n_eig: int = 5):
        super().__init__()
        self.kernel = kernel
        self.epsilon = epsilon
        self.n_eig = n_eig

    def fit(self, data: Tuple[np.ndarray, np.ndarray], **kwargs):
        gram_0 = self.kernel.gram(data[0])
        gram_1 = self.kernel.apply(*data)

        if self.epsilon > 0:
            reg = self.epsilon * np.eye(gram_0.shape[0])
        else:
            reg = 0
        A = np.linalg.pinv(gram_0 + reg, rcond=1e-15) @ gram_1
        eigenvalues, eigenvectors = np.linalg.eig(A)
        eigenvalues, eigenvectors = sort_by_norm(eigenvalues, eigenvectors)
        perron_frobenius_operator = eigenvectors
        koopman_operator = gram_0 @ eigenvectors

        self._model = KernelEDMDModel(perron_frobenius_operator, koopman_operator)

        return self


class KernelCCA(Model):

    def __init__(self, eigenvalues: np.ndarray, eigenvectors: np.ndarray):
        super().__init__()
        self.eigenvalues = eigenvalues
        self.eigenvectors = eigenvectors


class KernelCCAEstimator(Estimator):

    def __init__(self, kernel: Kernel, n_evs: int, epsilon: float = 1e-6):
        super().__init__()
        self.kernel = kernel
        self.n_evs = n_evs
        self.epsilon = epsilon

    def fit(self, data, **kwargs):
        gram_0 = self.kernel.gram(data[0])
        gram_t = self.kernel.gram(data[1])
        # center Gram matrices
        n = data[0].shape[0]
        I = np.eye(n)
        N = I - 1 / n * np.ones((n, n))
        G_0 = N @ gram_0 @ N
        G_1 = N @ gram_t @ N

        A = scipy.linalg.solve(G_0 + self.epsilon * I, G_0, assume_a='sym') \
            @ scipy.linalg.solve(G_1 + self.epsilon * I, G_1, assume_a='sym')

        eigenvalues, eigenvectors = np.linalg.eig(A)
        eigenvalues, eigenvectors = sort_by_norm(eigenvalues, eigenvectors)

        # determine effective rank m and perform low-rank approximations.
        if eigenvalues.shape[0] > self.n_evs:
            eigenvectors = eigenvectors[:, :self.n_evs]
            eigenvalues = eigenvalues[:self.n_evs]

        self._model = KernelCCA(eigenvalues, eigenvectors)
        return self
